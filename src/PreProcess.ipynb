{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from configparser import ConfigParser\n",
    "import pickle\n",
    "\n",
    "import enchant\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:(200, 2)\n",
      "Drop Dupicates:(200, 2)\n",
      "Drop Nulls: (200, 2)\n",
      "Drop Nulls: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "class PreProcess():\n",
    "\n",
    "    def __init__(self,obj_dict):\n",
    "        try:\n",
    "            self.data=pd.read_csv(obj_dict['input_data_path'] + obj_dict['input_filename'], encoding='latin1',header=0)\n",
    "\n",
    "        except:\n",
    "            print('Run first pre-processing configuration file (PreProcessConfiguration.py)')\n",
    "            \n",
    "        print(f\"Original:{self.data.shape}\")\n",
    "        \n",
    "        data_dd = self.data.drop_duplicates()\n",
    "        dd = data_dd.reset_index(drop=True)\n",
    "        print(f\"Drop Dupicates:{dd.shape}\")\n",
    "        \n",
    "        dd_dn = dd.dropna()\n",
    "        df = dd_dn.reset_index(drop=True)\n",
    "        print(f\"Drop Nulls: {df.shape}\")\n",
    "\n",
    "        self.lowercase=False\n",
    "        self.tokenized=False\n",
    "        self.punctuations=False\n",
    "        self.numberRemoval=False\n",
    "        self.classes=list(df['sentiment_class'])\n",
    "        self.reviews=list(df['text'])\n",
    "\n",
    "        if obj_dict['negation_handling']=='True': # converts dont to do not\n",
    "            self.negation_handling(obj_dict['appos'])\n",
    "        if obj_dict['remove_punctuation']=='True':# removes #!~ kind of litterals\n",
    "            self.remove_punctuations()\n",
    "        if obj_dict['number_removal']=='True': # removes numbers from text\n",
    "            self.number_removal()\n",
    "        if obj_dict['numbers_to_names']=='True': # converts 10 to ten (number to their english name)\n",
    "            self.numbers_to_names()\n",
    "        if obj_dict['stop_words_removal']=='True': # removes stop words\n",
    "            self.stop_words_removal()\n",
    "        if obj_dict['gibberish_word_removal']=='True': # removes non english and noun words\n",
    "            self.gibberish_word_removal()\n",
    "        if obj_dict['lemmatization']=='True': # lemmatization of words\n",
    "            self.lemmatization()\n",
    "        if obj_dict['convert_to_lowercase']=='True': # converting to lowercase\n",
    "            self.convert_to_lowercase()\n",
    "            \n",
    "        for i in range(len(self.reviews)): # removing extra spaces in the reviews\n",
    "            self.reviews[i] = re.sub(r\"\\s+\", \" \", self.reviews[i])\n",
    "            \n",
    "        # removing one letter words\n",
    "        self.tokenize_text()\n",
    "        temp=[]\n",
    "        for i in range(len(self.reviews)):\n",
    "            temp=[j for j in self.reviews[i] if len(j)>1]\n",
    "            self.reviews[i]=' '.join(j for j in temp)\n",
    "            temp=[]\n",
    "        self.tokenized=False\n",
    "        \n",
    "        if obj_dict['tokenization']=='True': # tokenization\n",
    "            self.tokenize_text()\n",
    "        \n",
    "            \n",
    "    def gibberish_word_removal(self):\n",
    "        '''except for nouns removes words which are not in english dictionary'''\n",
    "        \n",
    "        standard_dict = enchant.Dict(\"en_US\")\n",
    "            # extracting each data row one by one\n",
    "        for i in range(len(self.reviews)):\n",
    "            tokenized = sent_tokenize(self.reviews[i])\n",
    "            for j in tokenized:\n",
    "                # Word tokenizers is used to find the words\n",
    "                # and punctuation in a string\n",
    "                wordsList = nltk.word_tokenize(j)\n",
    "                # removing stop words from wordList\n",
    "                # Using a Tagger. Which is part-of-speech\n",
    "                # tagger or POS-tagger.\n",
    "                tagged = nltk.pos_tag(wordsList)\n",
    "                #print(tagged)         \n",
    "\n",
    "            tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "            words_list = tokenizer.tokenize(self.reviews[i])\n",
    "            for word in words_list:\n",
    "                a = 0\n",
    "                if word.isnumeric() == False:\n",
    "                    #check if the word is a proper noun\n",
    "                    for j in tagged:\n",
    "                        if j[0] == word and (j[1] == 'NNP' or j[1] == 'NNPS'):\n",
    "                            a = 1\n",
    "                            break\n",
    "                    if a != 1 and standard_dict.check(word) == False:\n",
    "                        #get suggestions for the input word from the standard dictionary\n",
    "                        self.reviews[i] = self.reviews[i].replace(word, ' ')\n",
    "            \n",
    "            self.tokenized=False\n",
    "\n",
    "    def number_removal(self):\n",
    "        '''removes numerics'''\n",
    "        if self.numberRemoval==False:\n",
    "            for i in range(len(self.reviews)):\n",
    "                self.reviews[i] = re.sub(r'\\w*\\d\\w*', ' ', self.reviews[i])\n",
    "            self.numberRemoval=True\n",
    "\n",
    "    def tokenize_text(self):\n",
    "    \n",
    "        if self.tokenized==False:\n",
    "            for i in range(len(self.reviews)):\n",
    "                self.reviews[i]=word_tokenize(self.reviews[i])\n",
    "            self.tokenized=True\n",
    "\n",
    "    def negation_handling(self,appos):\n",
    "\n",
    "        ''' converts dont to do not'''\n",
    "        self.convert_to_lowercase()\n",
    "        self.remove_punctuations()\n",
    "        for i in range(len(self.reviews)):\n",
    "            for j in appos.keys():\n",
    "                self.reviews[i]=self.reviews[i].replace(j+' ',appos[j]+' ')\n",
    "\n",
    "    def convert_to_lowercase(self):\n",
    "\n",
    "        '''converts the text in data to lowercase'''\n",
    "        if self.lowercase==False:\n",
    "            for i in range(len(self.classes)):\n",
    "                self.classes[i]=self.classes[i].lower()\n",
    "                self.reviews[i]=self.reviews[i].lower()\n",
    "            self.lowercase=True\n",
    "\n",
    "    def remove_punctuations(self):\n",
    "\n",
    "        '''removes punctuations in text  '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]_^`{|}~' '''\n",
    "        if self.punctuations==False:\n",
    "            for i in range(len(self.reviews)):\n",
    "                self.reviews[i] = self.reviews[i].translate(str.maketrans('', '', string.punctuation.replace('_','')))\n",
    "                self.punctuations=True\n",
    "\n",
    "    def numbers_to_names(self):\n",
    "\n",
    "        '''converts number(10) to its name(ten)'''\n",
    "        if self.numberRemoval==False:\n",
    "            self.convert_to_lowercase()\n",
    "            for i in range(len(self.reviews)):\n",
    "                m = re.findall(r'\\d+', self.reviews[i])\n",
    "                for j in m:\n",
    "                    try:\n",
    "                        x=num2words(j,lang='en_IN').replace('-',' ')\n",
    "                        self.reviews[i]=self.reviews[i].replace(j,x)\n",
    "                    except:\n",
    "                        print(f'for {i} review number exceeded that limit of abs')\n",
    "    \n",
    "    def stop_words_removal(self):\n",
    "        \n",
    "        self.convert_to_lowercase()\n",
    "        self.tokenize_text()\n",
    "        \n",
    "        my_stop_words = list(text.ENGLISH_STOP_WORDS) #importing stopwords\n",
    "        \n",
    "        #exclude the words from stop words which may effect sentiment of a sentence\n",
    "        exempt=['against','below','cannot','cant','couldnt','cry','fire','hasnt','never','not','nobody','nor'\n",
    "       ,'nothing','under','no','off','than']\n",
    "        \n",
    "        #include words which may be custom to your dataset\n",
    "        my_stop_words.extend(['url','https','http','com'])\n",
    "\n",
    "        for i in exempt:\n",
    "            my_stop_words.remove(i)\n",
    "            \n",
    "        #removing stopwords\n",
    "        for i in range(len(self.reviews)):\n",
    "            self.reviews[i] = [w for w in self.reviews[i] if not w in my_stop_words]\n",
    "            try:\n",
    "                self.reviews[i].remove(' ')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for i in range(len(self.reviews)):\n",
    "            self.reviews[i]=' '.join(w for w in self.reviews[i])+' '\n",
    "            \n",
    "        self.tokenized=False\n",
    "\n",
    "    def lemmatization(self):\n",
    "        \n",
    "        self.convert_to_lowercase()\n",
    "        for i in range(len(self.reviews)):\n",
    "            doc = nlp(self.reviews[i])\n",
    "            # Create list of tokens from given string\n",
    "            tokens = []\n",
    "            for token in doc:\n",
    "                tokens.append(token)\n",
    "\n",
    "            self.reviews[i] = \" \".join([token.lemma_ for token in doc])\n",
    "        \n",
    "        self.tokenized=False\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../conf/PreProcessConfiguration.py # running confie file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    \n",
    "    config = ConfigParser()\n",
    "    config.read('../conf/PreProcess.ini')\n",
    "\n",
    "    config2 = ConfigParser()\n",
    "    config2.read('../conf/collocations.ini')\n",
    "    \n",
    "    pfile = open('appos.pkl', 'rb')\n",
    "    apos = pickle.load(pfile)\n",
    "    pfile.close()\n",
    "    \n",
    "    obj_dict=dict()\n",
    "    \n",
    "    obj_dict[\"input_data_path\"] = config['Data']['input_data_path']\n",
    "    obj_dict[\"output_data_path\"] = config['Data']['output_data_path']\n",
    "    \n",
    "    obj_dict[\"input_filename\"] =config['Data']['input_filename']\n",
    "    obj_dict[\"raw_data\"] = config2['Collocations']['input_filename']\n",
    "    \n",
    "    obj_dict[\"negation_handling\"]=config['Data']['negation_handling']\n",
    "    obj_dict[\"remove_punctuation\"]=config['Data']['remove_punctuation']\n",
    "    obj_dict[\"numbers_to_names\"]=config['Data']['numbers_to_names']\n",
    "    obj_dict[\"stop_words_removal\"]=config['Data']['stop_words_removal']\n",
    "    obj_dict[\"lemmatization\"]=config['Data']['lemmatization']\n",
    "    obj_dict[\"convert_to_lowercase\"]=config['Data']['convert_to_lowercase']\n",
    "    obj_dict[\"tokenization\"]=config['Data']['tokenization']\n",
    "    obj_dict[\"number_removal\"]=config['Data']['number_removal']\n",
    "    obj_dict[\"gibberish_word_removal\"]=config['Data']['gibberish_word_removal']\n",
    "    obj_dict['appos']=apos\n",
    "    \n",
    "    import os\n",
    "    try:\n",
    "        os.mkdir(obj_dict['output_data_path'][:-1])\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    if not os.path.exists(obj_dict['output_data_path'] + obj_dict[\"raw_data\"].split('.')[0]+'_PreProcessed.csv'):\n",
    "        obj=PreProcess(obj_dict)\n",
    "        classes_df = pd.DataFrame(obj.classes)\n",
    "        reviews_df = pd.DataFrame(obj.reviews)\n",
    "        x=pd.concat([classes_df , reviews_df] , axis=1)\n",
    "        x.columns=['sentiment_class','text']\n",
    "        \n",
    "        x.replace('',np.nan,inplace=True)\n",
    "        x.dropna(subset=['text'],inplace=True)\n",
    "        x_new2 =x.reset_index(drop=True)\n",
    "        print(f\"Drop Nulls: {x_new2.shape}\")\n",
    "\n",
    "        x_new2.to_csv(obj_dict['output_data_path'] + obj_dict[\"raw_data\"].split('.')[0]+'_PreProcessed.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
