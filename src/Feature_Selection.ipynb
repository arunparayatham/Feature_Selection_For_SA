{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #used in stopwords removal\n",
    "stop_wrds=stopwords.words('english')\n",
    "stop_wrds.remove('not')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import scipy.stats as ss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorization():\n",
    "    #this class creates TFIDF vectors for the given set of documents\n",
    "    \n",
    "    def create_TFIDF_vectors(self, documents, n_grams, maximum_df=1.0, minimum_df=1, maximum_features=2000):\n",
    "        '''this function returns creates TFIDF scores matrix and features'''\n",
    "        if maximum_features != None:\n",
    "            vectorizer = TfidfVectorizer(stop_words=stop_wrds, ngram_range=n_grams, max_df=maximum_df, min_df=minimum_df, max_features=maximum_features)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(stop_words=stop_wrds, ngram_range=n_grams)\n",
    "            \n",
    "        #vectorizer = TfidfVectorizer(stop_words=stop_wrds, ngram_range=(1, 1))\n",
    "        tfidf_matrix=vectorizer.fit_transform(documents)\n",
    "\n",
    "        feature_index = [tfidf_matrix[i,:].nonzero()[1] for i in range(len(documents))]\n",
    "\n",
    "        feature_names=vectorizer.get_feature_names()\n",
    "        features = [[feature_names[j] for j in i] for i in feature_index]\n",
    "\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns = vectorizer.get_feature_names())\n",
    "        #tfidf_scores = zip([feature_names[i] for i in feature_index], [tfidf_matrix[0, x] for x in feature_index])\n",
    "\n",
    "        return tfidf_matrix,feature_index,features,feature_names,tfidf_df\n",
    "\n",
    "\n",
    "    def get_feature_data(self, data, n_grams, max_df, min_df, max_features):\n",
    "        '''this function returns features and their corresponding tfidf scores along with the target class'''\n",
    "    \n",
    "        classes=list(data['sentiment_class'])\n",
    "        reviews=list(data['text'])\n",
    "\n",
    "        tfidf_matrix,feature_index,features,feature_names,tfidf_df = self.create_TFIDF_vectors(reviews, n_grams, max_df, min_df, max_features)\n",
    "        features_df = pd.DataFrame(feature_names,columns=['features'])\n",
    "        #tfidf_df.to_csv(obj_dict['output_data_path'] + '/tfidf_scores.csv',index=False,header=tfidf_df.columns)\n",
    "\n",
    "        reviews_df = pd.DataFrame(reviews,columns=['review'])\n",
    "        classes_df = pd.DataFrame(classes,columns=['class'])\n",
    "\n",
    "        feature_data_df=pd.concat([reviews_df,tfidf_df,classes_df],axis=1)\n",
    "        #feature_data_df.to_csv('results/feature_data.csv',index=False,header=feature_data_df.columns)\n",
    "\n",
    "        return feature_data_df, tfidf_df  \n",
    "    \n",
    "    \n",
    "    def split_X_and_y(self, tfidf_df, feature_data_df):\n",
    "        ''' splits the target class column from columns of tfidf scores'''\n",
    "        y = pd.get_dummies(feature_data_df['class'], prefix = 'class')\n",
    "        if 'class' in y.columns.to_list():\n",
    "            y=y.drop(columns=['class'],axis=1)\n",
    "        X = tfidf_df\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OddsRatio():\n",
    "    # this class computes odds ratio\n",
    "    \n",
    "    def find_odds_ratio(self, X, containing, target_class):\n",
    "        ''' for a given target class (say, positive) this method computes the odds ratio of all features wrt the given target class'''\n",
    "        \n",
    "        odds_ratio_dict = {}\n",
    "        features = list(X.columns)\n",
    "        target_values = target_class.tolist()\n",
    "        for feature_index in range(len(features)):\n",
    "            a= list(X.iloc[:,feature_index])\n",
    "            b=containing[features[feature_index]]\n",
    "            \n",
    "            p = len([i for i in b if target_class[i]==1]) #no. of positive reviews with the feature word\n",
    "            q = len(b) #no. of reviews containing feature word\n",
    "            r = target_values.count(1) - p\n",
    "            s=len(a)-len(b) #no. of reviews not containing feature word\n",
    "\n",
    "            odds_ratio = (p/q) / (r/s)\n",
    "            odds_ratio_dict[X.columns[feature_index]]=odds_ratio\n",
    "\n",
    "        return odds_ratio_dict\n",
    "    \n",
    "    def odds_ratio_for_all_classes(self, X, y, containing):\n",
    "        ''' this method computes the odds ratio for all the classes individually'''\n",
    "        \n",
    "        odds_ratio_df = pd.DataFrame()\n",
    "        odds_ratio_df = pd.concat([odds_ratio_df,pd.DataFrame(X.columns.to_list())], axis=1)\n",
    "        \n",
    "        for i in range(len(y.columns)):\n",
    "            odds_ratio = self.find_odds_ratio(X,containing,y.iloc[:,i])\n",
    "            odds = pd.DataFrame(odds_ratio.values())\n",
    "            odds_ratio_df = pd.concat([odds_ratio_df, odds], axis=1)\n",
    "        \n",
    "        odds_ratio_df.columns = ['feature_word'] + y.columns.to_list()\n",
    "        \n",
    "        return odds_ratio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class Features():\n",
    "    ''' this class has different methods of feature selection'''\n",
    "    \n",
    "    # PEARSON CORRELATION\n",
    "    def cor_selector(self, X, y,num_feats):\n",
    "        ''' this method computes the pearson features for a given target class'''\n",
    "        \n",
    "        cor_list = []\n",
    "        feature_name = X.columns.tolist()\n",
    "\n",
    "        for i in X.columns.tolist(): # calculate the correlation with y for each feature\n",
    "            cor = np.corrcoef(X[i], y)[0, 1]\n",
    "            cor_list.append(cor)\n",
    "       \n",
    "        cor_list = [0 if np.isnan(i) else i for i in cor_list]  # replace NaN with 0\n",
    "        cor_feature = X.iloc[:,np.argsort(cor_list)[-num_feats:]].columns.tolist()[::-1] # feature name\n",
    "        cor_support = [True if i in cor_feature else False for i in feature_name] # feature selection? 0 for not select, 1 for select\n",
    "\n",
    "        return cor_support, cor_feature, cor_list\n",
    "    \n",
    "    def getPearsonCorrelationFeatures(self, X, y, num_feats,frequency_dict):\n",
    "        ''' this method returns the top num_feats pearson features from all the features'''\n",
    "        \n",
    "        pearson_features_df = pd.DataFrame()\n",
    "            \n",
    "        for i in range(len(y.columns)):\n",
    "            cor_support, cor_feature, cor_list = self.cor_selector(X,y.iloc[:,i],num_feats)\n",
    "            cor_feature_df = pd.DataFrame(cor_feature)\n",
    "            \n",
    "            frequency_list =[]\n",
    "            for feature in cor_feature:\n",
    "                frequency_list.append(frequency_dict[feature])\n",
    "            frequency_df = pd.DataFrame(frequency_list)\n",
    "            pearson_features_df = pd.concat([pearson_features_df, cor_feature_df, frequency_df],axis=1)\n",
    "            \n",
    "        #pearson_features_df.columns = y.columns.to_list()\n",
    "        columns_list = []\n",
    "        for column in y.columns.to_list():\n",
    "            columns_list.append(column)\n",
    "            columns_list.append(column+'_frequency')\n",
    "            \n",
    "        pearson_features_df.columns = columns_list\n",
    "        \n",
    "        return pearson_features_df\n",
    "    \n",
    "    # CHI SQUARE FEATURES\n",
    "    def getChiSquaredFeatures(self,X,y,no_features):\n",
    "        ''' this method computes the chi square features for a given target class'''\n",
    "        \n",
    "        X_norm = MinMaxScaler().fit_transform(X)\n",
    "        \n",
    "        chi_selector = SelectKBest(chi2, k='all')\n",
    "        chi_selector.fit(X_norm, y)\n",
    "\n",
    "        chi_support = chi_selector.get_support()\n",
    "        chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "\n",
    "        return chi_support, chi_feature\n",
    "    \n",
    "    def getChiSquaredCorrelationFeatures(self,X,y,num_feats,frequency_dict):\n",
    "        ''' this method returns the top num_feats chi square features from all the features'''\n",
    "        \n",
    "        chisquare_features_df = pd.DataFrame()\n",
    "        \n",
    "        for i in range(len(y.columns)):\n",
    "            chi_support, chi_feature = self.getChiSquaredFeatures(X,y.iloc[:,i],num_feats)\n",
    "            chi_feature_df = pd.DataFrame(chi_feature)\n",
    "            \n",
    "            frequency_list =[]\n",
    "            for feature in chi_feature:\n",
    "                frequency_list.append(frequency_dict[feature])\n",
    "            frequency_df = pd.DataFrame(frequency_list)\n",
    "            \n",
    "            chisquare_features_df = pd.concat([chisquare_features_df, chi_feature_df,frequency_df],axis=1)\n",
    "        \n",
    "        #chisquare_features_df.columns = y.columns.to_list()\n",
    "        columns_list = []\n",
    "        for column in y.columns.to_list():\n",
    "            columns_list.append(column)\n",
    "            columns_list.append(column+'_frequency')\n",
    "            \n",
    "        chisquare_features_df.columns = columns_list\n",
    "        \n",
    "        return chisquare_features_df\n",
    "    \n",
    "    # RECURSIVE FEATURE ELIMINATION (RFE) FEATURES\n",
    "    def findRFEFeatures(self,X,y,no_features):\n",
    "        ''' this method computes the Recursive Feature Elimination (RFE) features for a given target class'''\n",
    "        \n",
    "        X_norm = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "        rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=no_features, step=100, verbose=0)\n",
    "        rfe_selector.fit(X_norm, y)\n",
    "        rfe_support = rfe_selector.get_support()\n",
    "        rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
    "\n",
    "        return rfe_support, rfe_feature\n",
    "    \n",
    "    def getRFEFeatures(self,X,y,num_feats,frequency_dict):\n",
    "        ''' this method returns the top num_feats Recursive Feature Elimination (RFE) features from all the features'''\n",
    "        \n",
    "        rfe_features_df = pd.DataFrame()\n",
    "            \n",
    "        for i in range(len(y.columns)):\n",
    "            rfe_support, rfe_feature = self.findRFEFeatures(X,y.iloc[:,i],num_feats)\n",
    "            rfe_feature_df = pd.DataFrame(rfe_feature)\n",
    "            frequency_list =[]\n",
    "            for feature in rfe_feature:\n",
    "                frequency_list.append(frequency_dict[feature])\n",
    "            frequency_df = pd.DataFrame(frequency_list)\n",
    "            rfe_features_df = pd.concat([rfe_features_df, rfe_feature_df,frequency_df],axis=1)\n",
    "        \n",
    "        #rfe_features_df.columns = y.columns.to_list()\n",
    "        columns_list = []\n",
    "        for column in y.columns.to_list():\n",
    "            columns_list.append(column)\n",
    "            columns_list.append(column+'_frequency')\n",
    "            \n",
    "        rfe_features_df.columns = columns_list\n",
    "        \n",
    "        return rfe_features_df\n",
    "    \n",
    "    # LASSO REGRESSION (LR) FEATURES\n",
    "    def findLRFeatures(self,X,y,no_features):\n",
    "        ''' this method computes the lasso features for a given target class'''\n",
    "        \n",
    "        X_norm = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "        embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\"), max_features=no_features)\n",
    "        embeded_lr_selector.fit(X_norm, y)\n",
    "\n",
    "        embeded_lr_support = embeded_lr_selector.get_support()\n",
    "        embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "\n",
    "        return embeded_lr_support, embeded_lr_feature\n",
    "    \n",
    "    def getLRFeatures(self,X,y,num_feats,frequency_dict):\n",
    "        ''' this method returns the top num_feats Lasso Regression (LR) features from all the features'''\n",
    "        \n",
    "        lr_features_df = pd.DataFrame()\n",
    "            \n",
    "        for i in range(len(y.columns)):\n",
    "            lr_support, lr_feature = self.findLRFeatures(X,y.iloc[:,i],num_feats)\n",
    "            lr_feature_df = pd.DataFrame(lr_feature)\n",
    "            frequency_list =[]\n",
    "            for feature in lr_feature:\n",
    "                frequency_list.append(frequency_dict[feature])\n",
    "            frequency_df = pd.DataFrame(frequency_list)\n",
    "            lr_features_df = pd.concat([lr_features_df, lr_feature_df,frequency_df],axis=1)\n",
    "        \n",
    "        #lr_features_df.columns = y.columns.to_list()\n",
    "        columns_list = []\n",
    "        for column in y.columns.to_list():\n",
    "            columns_list.append(column)\n",
    "            columns_list.append(column+'_frequency')\n",
    "            \n",
    "        lr_features_df.columns = columns_list\n",
    "        \n",
    "        return lr_features_df\n",
    "    \n",
    "    # RANDOM FOREST CLASSIFIER (RFC) FEATURES\n",
    "    def findRFCFeatures(self,X,y,no_features):\n",
    "        ''' this method computes the random forest classifier features for a given target class'''\n",
    "        \n",
    "        embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=10), max_features=no_features)\n",
    "        embeded_rf_selector.fit(X, y)\n",
    "\n",
    "        embeded_rf_support = embeded_rf_selector.get_support()\n",
    "        embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "\n",
    "        return embeded_rf_support, embeded_rf_feature\n",
    "    \n",
    "    def getRFCFeatures(self,X,y,num_feats,frequency_dict):\n",
    "        ''' this method returns the top num_feats Random Forest Classifier (RFC) features from all the features'''\n",
    "        \n",
    "        rfc_features_df = pd.DataFrame()\n",
    "            \n",
    "        for i in range(len(y.columns)):\n",
    "            rfc_support, rfc_feature = self.findRFCFeatures(X,y.iloc[:,i],num_feats)\n",
    "            rfc_feature_df = pd.DataFrame(rfc_feature)\n",
    "            frequency_list =[]\n",
    "            for feature in rfc_feature:\n",
    "                frequency_list.append(frequency_dict[feature])\n",
    "            frequency_df = pd.DataFrame(frequency_list)\n",
    "            rfc_features_df = pd.concat([rfc_features_df, rfc_feature_df,frequency_df],axis=1)\n",
    "        \n",
    "        #rfc_features_df.columns = y.columns.to_list()\n",
    "        columns_list = []\n",
    "        for column in y.columns.to_list():\n",
    "            columns_list.append(column)\n",
    "            columns_list.append(column+'_frequency')\n",
    "            \n",
    "        rfc_features_df.columns = columns_list\n",
    "        \n",
    "        return rfc_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection():\n",
    "    ''' this class finds features from data using different methods '''\n",
    "    \n",
    "    def __init__(self, object_dict):\n",
    "        \n",
    "        self.input_data_path = object_dict['input_data_path']\n",
    "        self.output_data_path = object_dict['output_data_path']\n",
    "        \n",
    "        self.input_filename = object_dict['input_filename']\n",
    "        self.raw_data = object_dict['raw_data']\n",
    "        \n",
    "        #print(object_dict['preprocessed_data'])\n",
    "        self.preprocessed_data = pd.read_csv(object_dict['preprocessed_data'])\n",
    "        \n",
    "        if not os.path.exists(object_dict['preprocessed_data']):\n",
    "            raise Exception('------Perform preprocessing before performing Feature Selection!-----')\n",
    "            \n",
    "            \n",
    "        self.number_of_features = int(object_dict['no_of_features'])\n",
    "        import ast\n",
    "        self.n_gram_range = ast.literal_eval(object_dict['n_gram_range'])\n",
    "        \n",
    "        self.max_df = ast.literal_eval(object_dict['max_df'])\n",
    "        self.min_df = ast.literal_eval(object_dict['min_df'])\n",
    "        self.max_features = ast.literal_eval(object_dict['max_features'])\n",
    "        \n",
    "        vectorization = Vectorization()\n",
    "        \n",
    "        self.feature_data_df, self.tfidf_df = vectorization.get_feature_data(self.preprocessed_data, self.n_gram_range,self.max_df, self.min_df, self.max_features)\n",
    "        \n",
    "        self.tfidf_df.to_csv(object_dict['output_data_path'] + '/tfidf_scores.csv',index=False,header=self.tfidf_df.columns)\n",
    "        self.feature_data_df.to_csv(object_dict['output_data_path'] + '/feature_data.csv',index=False,header=self.feature_data_df.columns)\n",
    "        \n",
    "        self.X, self.y = vectorization.split_X_and_y(self.tfidf_df, self.feature_data_df)\n",
    "        print(len(self.X.columns),self.max_features)\n",
    "        \n",
    "        self.indices = self.store_indices(self.X)\n",
    "        self.frequency_dict = self.count_frequency(self.indices,object_dict)\n",
    "        \n",
    "        if len(self.X.columns)<int(self.max_features):\n",
    "            print('error')\n",
    "            #raise Exception('---- Selected number of features exceeded the total number of features ----\\n Total features : %s \\nMax features : %s'%len(self.X.columns) %self.max_features)\n",
    "        \n",
    "        features = Features()\n",
    "        if object_dict['pearson']=='True':\n",
    "            self.pearson_features_df = features.getPearsonCorrelationFeatures(self.X, self.y, self.number_of_features, self.frequency_dict)\n",
    "            self.pearson_features_df.to_csv(object_dict['output_data_path'] + 'pearson_features.csv',index=False)\n",
    "        \n",
    "        if object_dict['chi_square']=='True':\n",
    "            self.chisquare_features_df = features.getChiSquaredCorrelationFeatures(self.X, self.y, self.number_of_features, self.frequency_dict)\n",
    "            self.chisquare_features_df.to_csv(object_dict['output_data_path'] + 'chisquare_features.csv',index=False)\n",
    "        \n",
    "        if object_dict['rfe']=='True':\n",
    "            self.rfe_features_df = features.getRFEFeatures(self.X, self.y, self.number_of_features, self.frequency_dict)\n",
    "            self.rfe_features_df.to_csv(object_dict['output_data_path'] + 'rfe_features.csv',index=False)\n",
    "        \n",
    "        if object_dict['lr']=='True':\n",
    "            self.lr_features_df = features.getLRFeatures(self.X, self.y, self.number_of_features, self.frequency_dict)\n",
    "            self.lr_features_df.to_csv(object_dict['output_data_path'] + 'lr_features.csv',index=False)\n",
    "        \n",
    "        if object_dict['rfc']=='True':\n",
    "            self.rfc_features_df = features.getRFCFeatures(self.X, self.y, self.number_of_features, self.frequency_dict)\n",
    "            self.rfc_features_df.to_csv(object_dict['output_data_path'] + 'rfc_features.csv',index=False)\n",
    "        \n",
    "        odds_ratio = OddsRatio()\n",
    "        \n",
    "        if object_dict['odds_ratio']=='True':\n",
    "            self.odds_ratio_df = odds_ratio.odds_ratio_for_all_classes(self.X, self.y, self.indices)\n",
    "            self.odds_ratio_df.to_csv(object_dict['output_data_path'] + 'odds_ratio.csv',index = False)\n",
    "        \n",
    "    def store_indices(self,X):\n",
    "        ''' this method finds the indices of documents containing each feature\n",
    "            eg - indices['beautiful'] = [2,43,87,92]\n",
    "        '''\n",
    "        \n",
    "        indices = {}\n",
    "        features = list(X.columns)\n",
    "        for feature_index in range(len(features)):\n",
    "            a = list(X.iloc[:,feature_index])\n",
    "            containing = [i for i,x in enumerate(a) if x!=0]\n",
    "            indices[features[feature_index]] = containing\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def count_frequency(self,containing,obj_dict):\n",
    "        ''' this method counts the occurences of each feature in the corpus'''\n",
    "        \n",
    "        features = list(pd.read_csv(object_dict['output_data_path'] + 'tfidf_scores.csv').columns)\n",
    "        reviews = list(pd.read_csv(object_dict['preprocessed_data'])['text'])\n",
    "        \n",
    "        counts_dict = {}\n",
    "        for i in features:\n",
    "            indices = containing[i]\n",
    "            count =0\n",
    "            for j in indices:\n",
    "                count+=reviews[j].count(i)\n",
    "            counts_dict[i] = count\n",
    "\n",
    "        df = pd.DataFrame.from_dict(counts_dict,orient='index')\n",
    "        words_df = pd.DataFrame(counts_dict.keys())\n",
    "        counts_df = pd.DataFrame(counts_dict.values())\n",
    "\n",
    "        frequency_df = pd.concat([words_df, counts_df],axis=1)\n",
    "        frequency_df.columns = ['word','frequency']\n",
    "        frequency_df.to_csv(object_dict['output_data_path'] + object_dict['raw_data'].split('.')[0]+'_word_frequency.csv',index=False)\n",
    "        \n",
    "        return counts_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../conf/FeatureSelectionConfiguration.py \n",
    "# this creates the .ini file for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_data_path': '../results/PreProcessing_Results/', 'output_data_path': '../results/FeatureSelection_Results/', 'input_filename': '_PreProcessed.csv', 'raw_data': 'IMDBtrain.csv', 'preprocessed_data': '../results/PreProcessing_Results/IMDBtrain_PreProcessed.csv', 'no_of_features': '200', 'n_gram_range': '(1, 2)', 'idf_weighing': 'False', 'max_df': '200', 'min_df': '1', 'max_features': '200', 'pearson': 'True', 'chi_square': 'True', 'rfe': 'True', 'lr': 'True', 'rfc': 'True', 'odds_ratio': 'True'}\n",
      "200 200\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from configparser import ConfigParser\n",
    "    \n",
    "    config = ConfigParser()\n",
    "    config.read('../conf/feature_selection.ini') #read feature selection configuration file\n",
    "    \n",
    "    config2 = ConfigParser()\n",
    "    config2.read('../conf/Collocations.ini') #read collocations configuration file\n",
    "    \n",
    "    #read the values from configuration and store in a dictionary for further usage\n",
    "    object_dict = dict()\n",
    "\n",
    "    object_dict['input_data_path'] = config['FeatureSelection']['input_data_path']\n",
    "    object_dict['output_data_path'] = config['FeatureSelection']['output_data_path']\n",
    "    \n",
    "    object_dict['input_filename'] = config['FeatureSelection']['input_filename']\n",
    "    object_dict['raw_data'] = config2['Collocations']['input_filename']\n",
    "    \n",
    "    object_dict['preprocessed_data'] = object_dict['input_data_path'] + object_dict['raw_data'].split('.')[0] + '_PreProcessed.csv'\n",
    "    \n",
    "    object_dict['no_of_features'] = config['FeatureSelection']['number_of_features']\n",
    "    \n",
    "    object_dict['n_gram_range'] = config['FeatureSelection']['n_gram_range']\n",
    "    object_dict['idf_weighing'] = config['FeatureSelection']['idf_weighing']\n",
    "    object_dict['max_df'] = config['FeatureSelection']['max_df']\n",
    "    object_dict['min_df'] = config['FeatureSelection']['min_df']\n",
    "    object_dict['max_features'] = config['FeatureSelection']['max_features']\n",
    "    \n",
    "    object_dict['pearson'] = config['FeatureSelection']['pearson_correlation']\n",
    "    object_dict['chi_square'] = config['FeatureSelection']['chi_square_correlation']\n",
    "    object_dict['rfe'] = config['FeatureSelection']['recursive_feature_elimination']\n",
    "    object_dict['lr'] = config['FeatureSelection']['lasso_regression']\n",
    "    object_dict['rfc'] = config['FeatureSelection']['random_forest_classifier']\n",
    "    object_dict['odds_ratio'] = config['FeatureSelection']['odds_ratio']\n",
    "    \n",
    "    print(object_dict)\n",
    "    \n",
    "    import os\n",
    "    try:\n",
    "        os.mkdir(object_dict['output_data_path'][:-1])\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    result = FeatureSelection(object_dict) #perform feature selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
