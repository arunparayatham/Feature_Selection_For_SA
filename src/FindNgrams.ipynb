{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********REQUIRED PACKAGES**********\n",
    "\n",
    "#!pip install spacy\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#or\n",
    "#conda install -c conda-forge spacy-model-en_core_web_sm\n",
    "\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\athik\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import gensim \n",
    "import logging\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collocations():\n",
    "    ''' this class contains methods to extract collocations from the corpus'''\n",
    "    \n",
    "    #GENSIM TOKENIZATION (SIMPLE)\n",
    "    def nltk_sentence_tokenizer(self, x):\n",
    "        ''' returns sentence tokenization for a given paragraph'''\n",
    "        return sent_tokenize(x)\n",
    "\n",
    "    def getPOStags(self, sent):\n",
    "        ''' returns the POS tags for all the words in a given sentence'''\n",
    "        pos_tags = []\n",
    "        for token in sent:\n",
    "            pos_tags.append(token.pos_)\n",
    "        return pos_tags\n",
    "\n",
    "    def read_input_token(self, corpus):\n",
    "        ''' reads each review and replaces any characters other than alphabets with a blank space'''\n",
    "        result =[]\n",
    "        for i, para in enumerate(corpus):\n",
    "            if i % 10 == 0:\n",
    "                print(\"Gensim: Processing para = \", i)\n",
    "            sentences = self.nltk_sentence_tokenizer(para)\n",
    "            for sent in sentences:\n",
    "                sent_clean = ''\n",
    "                for token in nlp(sent):\n",
    "                    str = re.sub(r\"[^a-zA-Z]+\", ' ', token.text)\n",
    "                    sent_clean = sent_clean + ' ' + str\n",
    "                temp = gensim.utils.simple_preprocess(sent_clean)\n",
    "                #yield gensim.utils.simple_preprocess(sent_clean) \n",
    "                '''Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.'''\n",
    "            result.append(temp)\n",
    "        return result\n",
    "    \n",
    "    # RETOKENIZATION USING SPACY\n",
    "    def processNounPhrases(self, noun_phrases):\n",
    "        ''' removes determinants from the sentence '''\n",
    "        processed_NounPhrases = []\n",
    "        for i,noun_phrase in enumerate(noun_phrases):\n",
    "            processed_nounPhrase = ''\n",
    "            sent = nlp(noun_phrase)\n",
    "            for token in sent:\n",
    "                #Remove \"the\", \"a\", \"an\" etc\n",
    "                if token.pos_ != 'DET':\n",
    "                    if token.pos_ == 'NUM':\n",
    "                        processed_nounPhrase = processed_nounPhrase + ' ' + token.text\n",
    "                    elif not token.text.isspace():\n",
    "                        processed_nounPhrase = processed_nounPhrase + ' ' + token.text\n",
    "            processed_NounPhrases.append(processed_nounPhrase.lstrip())\n",
    "        return processed_NounPhrases \n",
    "\n",
    "    def get_tokens_retokenization(self, sent):\n",
    "        ''' return the ngram tokenization for each sentence '''\n",
    "        sentence = sent.lower()\n",
    "        # Remove extra characters\n",
    "        sent_clean = ''\n",
    "        for token in nlp(sentence):\n",
    "            str = re.sub(r\"[^a-zA-Z]+\", ' ', token.text)\n",
    "            sent_clean = sent_clean + ' ' + str\n",
    "        sentence_doc = nlp(sent_clean)\n",
    "\n",
    "        spans = list(sentence_doc.ents) + list(sentence_doc.noun_chunks)  # collect nodes\n",
    "        spans = spacy.util.filter_spans(spans) # remove duplicates\n",
    "\n",
    "        with sentence_doc.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span) for span in spans]\n",
    "\n",
    "        tokens_spacy_retokenize = []\n",
    "        for token in sentence_doc:\n",
    "            tokens_spacy_retokenize.append(token.text)\n",
    "\n",
    "        tokens_spacy_retokenize_process = self.processNounPhrases(tokens_spacy_retokenize)\n",
    "        return tokens_spacy_retokenize_process\n",
    "\n",
    "    def read_input_retokenization(self, corpus):\n",
    "        ''' return ngram tokens for the corpus'''\n",
    "        \n",
    "        result = []\n",
    "        for i, para in enumerate(corpus):\n",
    "            if i % 10 == 0:\n",
    "                print(\"Retokenization: Processing para = \", i)\n",
    "            sentences = self.nltk_sentence_tokenizer(para) #sentence tokenization\n",
    "            for sent in sentences:\n",
    "                #yield get_tokens_retokenization(sent)\n",
    "                temp = self.get_tokens_retokenization(sent)\n",
    "            result.append(temp)\n",
    "        return result\n",
    "    \n",
    "    def find_ngrams_dict(self, ngrams):\n",
    "        '''returns a dict containing the ngrams (other than unigrams) of each document'''\n",
    "        ngrams_dict = {}\n",
    "        for i in range(len(ngrams)):\n",
    "            temp = [i for i in ngrams[i] if len(i.split())>1]\n",
    "            ngrams_dict[i] = temp\n",
    "        return ngrams_dict\n",
    "\n",
    "    def replace_ngrams(self, reviews,ngrams_dict):\n",
    "        ''' replace the original text in the corpus with the ngrams joined by '_' '''\n",
    "        \n",
    "        new_reviews = []\n",
    "        for i in range(len(reviews)):\n",
    "            temp = reviews[i].lower()\n",
    "            for j in ngrams_dict[i]:\n",
    "                if j in temp:\n",
    "                    new_str = j.replace(' ','_')\n",
    "                    temp = temp.replace(j,new_str)\n",
    "            new_reviews.append(temp)\n",
    "        return new_reviews\n",
    "\n",
    "    def make_new_csv(self, new_reviews, filename, sentiment_class):\n",
    "        ''' creates a .csv file with the ngrams replaced '''\n",
    "        \n",
    "        new_reviews_df = pd.DataFrame(new_reviews)\n",
    "        new_reviews_df.columns=['text']\n",
    "        sentiment_class_df = pd.DataFrame(sentiment_class)\n",
    "        sentiment_class_df.columns=['sentiment_class']\n",
    "        #sentiment_class_df = pd.read_csv('IMDBtrain.csv',encoding='latin')['sentiment_class'][:200]\n",
    "        new_reviews_df = pd.concat([new_reviews_df,sentiment_class_df],axis=1)\n",
    "\n",
    "        new_reviews_df.to_csv(filename,index=False,header=True)\n",
    "\n",
    "        return new_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point wise Mutual Information (PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMI():\n",
    "    ''' this class contains methods to compute PMI scores for each ngram in the corpus '''\n",
    "    \n",
    "    def find_all_ngrams_from_corpus(self, reviews_list):\n",
    "        ngrams_list = []\n",
    "        for review in reviews_list:\n",
    "            for word in word_tokenize(review):\n",
    "                if '_' in word:\n",
    "                    ngrams_list.append(word)\n",
    "        ngrams_list = list(set(ngrams_list))\n",
    "        return ngrams_list\n",
    "\n",
    "    def get_frequency_of_all_ngrams(self, ngrams_list, reviews_list):\n",
    "        freq_dict_ngram = {}\n",
    "        i=0\n",
    "        for ngram in ngrams_list:\n",
    "            #print(i)\n",
    "            count=0\n",
    "            for review in reviews_list:\n",
    "                count+=review.count(ngram)\n",
    "            freq_dict_ngram[ngram] = count\n",
    "            i+=1\n",
    "        return freq_dict_ngram\n",
    "\n",
    "    def get_frequency_of_all_corpus_words(self, raw_data):\n",
    "        freq_dict_word = {}\n",
    "\n",
    "        for review in raw_data:\n",
    "            tokens = word_tokenize(review)\n",
    "            for token in tokens:\n",
    "                if token in freq_dict_word:\n",
    "                    freq_dict_word[token]+=1\n",
    "                else:\n",
    "                    freq_dict_word[token]=1\n",
    "\n",
    "        return freq_dict_word\n",
    "\n",
    "    def get_frequency_of_all_corpus_words(self, raw_data):\n",
    "        freq_dict_word = {}\n",
    "        for review in raw_data:\n",
    "            tokens = word_tokenize(review)\n",
    "            for token in tokens:\n",
    "                if token in freq_dict_word:\n",
    "                    freq_dict_word[token]+=1\n",
    "                else:\n",
    "                    freq_dict_word[token]=1\n",
    "\n",
    "        return freq_dict_word\n",
    "\n",
    "    # (log(the future) / (log(the)*log(future)) )* len(dict)\n",
    "\n",
    "    def get_pmi_score_for_ngram(self, freq_dict_word,freq_dict_ngram, ngram):\n",
    "        ''' computes PMI score for an n-gram '''\n",
    "        tokens = ngram.split('_')\n",
    "        if ngram not in freq_dict_ngram:\n",
    "            return 0\n",
    "\n",
    "        freq_ngram = freq_dict_ngram[ngram]\n",
    "        word_freqs = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in freq_dict_word:\n",
    "                return 0\n",
    "            word_freqs.append(freq_dict_word[token])\n",
    "\n",
    "        product = 1\n",
    "        for element in word_freqs:\n",
    "            product*=element\n",
    "\n",
    "        return math.log ( (freq_ngram * len(freq_dict_word) / product ) , 2 )\n",
    "\n",
    "    def get_pmi_scores_for_all_ngrams(self, freq_dict_ngram, freq_dict_word):\n",
    "        ''' computes PMI scores for all the ngrams in the corpus '''\n",
    "        pmi_scores_dict = {}\n",
    "        for word in freq_dict_ngram:\n",
    "            pmi_scores_dict[word] = (freq_dict_ngram[word], self.get_pmi_score_for_ngram(freq_dict_word, freq_dict_ngram, word))\n",
    "        pmi_scores_dict = dict(sorted(pmi_scores_dict.items(), key=lambda item: item[1][1], reverse=True))\n",
    "\n",
    "        return pmi_scores_dict\n",
    "\n",
    "    def filter_ngrams_by_pmi(self, pmi_scores_df, pmi_threshold):\n",
    "        ''' return only those ngrams having PMI score above the mentioned PMI_threshold value '''\n",
    "        new_df = pmi_scores_df.sort_values(by='Frequency',ascending=False)\n",
    "        #new_df\n",
    "\n",
    "        pmi_list = list(new_df['PMI'])\n",
    "        temp = [i for i in range(len(pmi_list)) if pmi_list[i]>pmi_threshold] #0 value can be replaced by any threshold\n",
    "        #temp\n",
    "\n",
    "        filtered_df = new_df.iloc[temp,:]\n",
    "        return filtered_df\n",
    "\n",
    "    def create_csv_from_dict(self, data,filename,column_names):\n",
    "        ''' creates .csv file for the given dictionary data '''\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(data, orient = 'index')\n",
    "        df.columns = column_names\n",
    "        df.to_csv(filename,header=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindNgrams():\n",
    "    ''' this class contains the flow of execution for finding ngrams in the corpus '''\n",
    "    def __init__(self,object_dict):\n",
    "        \n",
    "        self.input_data_path = object_dict['input_data_path']\n",
    "        self.output_data_path = object_dict['output_data_path']\n",
    "        \n",
    "        self.filename = object_dict['input_filename']\n",
    "        self.encoding = object_dict['encoding']\n",
    "        \n",
    "        self.pmi_threshold = int(object_dict['PMI_threshold'])\n",
    "        self.ngram_results_dir = object_dict['ngram_results_dir']\n",
    "        self.pmi_results_dir = object_dict['pmi_results_dir']\n",
    "        \n",
    "        self.ngram_processed_data = object_dict['ngram_processed_data']\n",
    "        \n",
    "        self.reviews = list(pd.read_csv(self.input_data_path + self.filename, encoding=self.encoding)['text'])\n",
    "        self.sentiment_class = list(pd.read_csv(self.input_data_path + self.filename, encoding=self.encoding)['sentiment_class'])\n",
    "        \n",
    "        self.reviews = self.reviews[:100] + self.reviews[-100:]\n",
    "        self.sentiment_class = self.sentiment_class[:100] + self.sentiment_class[-100:]\n",
    "        \n",
    "        #---------------------- Ngrams main function ------------------------\n",
    "        import os.path\n",
    "        \n",
    "        if not os.path.exists(self.output_data_path + self.ngram_results_dir +'/NgramProcessedData.csv'):\n",
    "            \n",
    "            collocations = Collocations()\n",
    "\n",
    "            self.unigrams = list(collocations.read_input_token(self.reviews))\n",
    "            self.unigrams_df = pd.DataFrame(self.unigrams)\n",
    "            self.unigrams_df.to_csv(self.output_data_path + self.ngram_results_dir +'/WordTokensForEachDoc.csv',index=False) #stores all unigrams in the corpus document wise (simple word tokenization)\n",
    "            #unigrams\n",
    "\n",
    "            self.all_unigrams = []\n",
    "            for i in self.unigrams:\n",
    "                self.all_unigrams.extend(i)\n",
    "\n",
    "            self.unique_unigrams = list(set(self.all_unigrams))\n",
    "            self.unique_unigrams_df = pd.DataFrame(self.unique_unigrams)\n",
    "            self.unique_unigrams_df.to_csv(self.output_data_path + self.ngram_results_dir +'/UnigramsOfCorpus.csv',index=False) #list of all the unigrams in the corpus (no duplicates)\n",
    "\n",
    "            self.ngrams = list(collocations.read_input_retokenization(self.reviews))\n",
    "\n",
    "            self.new_ngrams = []\n",
    "            for i in self.ngrams:\n",
    "                self.new_ngrams.append(list(filter(None,i)))\n",
    "\n",
    "            self.new_ngrams_df = pd.DataFrame([self.new_ngrams]).transpose()\n",
    "            self.new_ngrams_df.columns=['Ngram tokens']\n",
    "            self.new_ngrams_df.to_csv(self.output_data_path + self.ngram_results_dir +'/Ngrams.csv',index=False) #stores all ngrams document wise\n",
    "\n",
    "            self.ngrams_dict = collocations.find_ngrams_dict(self.ngrams)\n",
    "            self.ngrams_dict_df = pd.DataFrame.from_dict(self.ngrams_dict, orient='index')\n",
    "            self.ngrams_dict_df.to_csv(self.output_data_path + self.ngram_results_dir +'/NgramsDictForEachDoc.csv',index=False) #stores all ngrams except unigrams (document wise)\n",
    "            #ngrams_dict\n",
    "\n",
    "            self.ngram_replaced_reviews = collocations.replace_ngrams(self.reviews, self.ngrams_dict)\n",
    "            #ngram_replaced_reviews\n",
    "\n",
    "            self.ngram_replaced_reviews_df = collocations.make_new_csv(self.ngram_replaced_reviews, self.output_data_path + self.ngram_results_dir +'/NgramProcessedData.csv', self.sentiment_class) #replaces ngrams in the original corpus with _\n",
    "            #ngram_replaced_reviews_df\n",
    "        \n",
    "        else:\n",
    "            self.ngram_replaced_reviews_df = pd.read_csv(self.output_data_path + self.ngram_results_dir +'/NgramProcessedData.csv') #replaced ngrams in the original corpus with _\n",
    "            #ngram_replaced_reviews_df\n",
    "        \n",
    "        #------------------------------ PMI main function -----------------------------\n",
    "        \n",
    "        if not os.path.exists(self.output_data_path + self.pmi_results_dir +'/FilteredNgramsByPMI.csv'):\n",
    "        \n",
    "            pmi = PMI()\n",
    "\n",
    "            self.raw_data = self.reviews\n",
    "            self.raw_data = [i.lower() for i in self.raw_data]\n",
    "\n",
    "            self.reviews_list = list(pd.read_csv(self.output_data_path + self.ngram_results_dir +'/NgramProcessedData.csv')['text']) #reviews after finding ngrams\n",
    "            #reviews_list\n",
    "\n",
    "            self.ngrams_list = pmi.find_all_ngrams_from_corpus(self.reviews_list)\n",
    "            self.ngrams_list_df = pd.DataFrame(self.ngrams_list).to_csv(self.output_data_path + self.pmi_results_dir +'/Ngrams.csv',index=False) #stores all the ngrams from corpus\n",
    "\n",
    "            self.freq_dict_ngram = pmi.get_frequency_of_all_ngrams(self.ngrams_list, self.reviews_list)\n",
    "            self.freq_dict_ngram_df = pmi.create_csv_from_dict(self.freq_dict_ngram, self.output_data_path + self.pmi_results_dir +'/NgramFrequencies.csv',['Frequency']) #stores the frequency of each ngram\n",
    "\n",
    "            self.freq_sorted_dict = dict(sorted(self.freq_dict_ngram.items(), key=lambda item: item[1], reverse=True))\n",
    "            #freq_sorted_dict\n",
    "\n",
    "            self.freq_dict_word = pmi.get_frequency_of_all_corpus_words(self.raw_data)\n",
    "            self.freq_dict_word_df = pmi.create_csv_from_dict(self.freq_dict_word, self.output_data_path + self.pmi_results_dir +'/WordFrequencies.csv',['Frequency']) #stores the frequency of each unigram in the corpus\n",
    "            #freq_dict_word\n",
    "\n",
    "            self.pmi_scores_dict = pmi.get_pmi_scores_for_all_ngrams(self.freq_dict_ngram, self.freq_dict_word)\n",
    "            self.pmi_scores_df = pmi.create_csv_from_dict(self.pmi_scores_dict, self.output_data_path + self.pmi_results_dir +'/NgramPMIScores.csv',['Frequency','PMI']) #stores the frequency and PMI score for each ngram\n",
    "            #pmi_scores_dict\n",
    "\n",
    "            self.filtered_pmi_df = pmi.filter_ngrams_by_pmi(self.pmi_scores_df, self.pmi_threshold)\n",
    "            self.filtered_pmi_df.to_csv(self.output_data_path + self.pmi_results_dir +'/FilteredNgramsByPMI.csv')\n",
    "            #filtered_pmi_df\n",
    "        \n",
    "        else:\n",
    "            self.filtered_pmi_df = pd.read_csv(self.output_data_path + self.pmi_results_dir +'/FilteredNgramsByPMI.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run configuration file (CollocationsConfiguration.py)\n",
    "%run ../conf/CollocationsConfiguration.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_data_path': '../data/', 'output_data_path': '../results/Collocations_Results/', 'input_filename': 'IMDBtrain.csv', 'encoding': 'latin', 'ngram_results_dir': 'Ngram_Results', 'pmi_results_dir': 'PMI_Results', 'PMI_threshold': '0', 'ngram_processed_data': 'IMDBtrain_NgramsProcessed.csv'}\n",
      "Gensim: Processing para =  0\n",
      "Gensim: Processing para =  10\n",
      "Gensim: Processing para =  20\n",
      "Gensim: Processing para =  30\n",
      "Gensim: Processing para =  40\n",
      "Gensim: Processing para =  50\n",
      "Gensim: Processing para =  60\n",
      "Gensim: Processing para =  70\n",
      "Gensim: Processing para =  80\n",
      "Gensim: Processing para =  90\n",
      "Gensim: Processing para =  100\n",
      "Gensim: Processing para =  110\n",
      "Gensim: Processing para =  120\n",
      "Gensim: Processing para =  130\n",
      "Gensim: Processing para =  140\n",
      "Gensim: Processing para =  150\n",
      "Gensim: Processing para =  160\n",
      "Gensim: Processing para =  170\n",
      "Gensim: Processing para =  180\n",
      "Gensim: Processing para =  190\n",
      "Retokenization: Processing para =  0\n",
      "Retokenization: Processing para =  10\n",
      "Retokenization: Processing para =  20\n",
      "Retokenization: Processing para =  30\n",
      "Retokenization: Processing para =  40\n",
      "Retokenization: Processing para =  50\n",
      "Retokenization: Processing para =  60\n",
      "Retokenization: Processing para =  70\n",
      "Retokenization: Processing para =  80\n",
      "Retokenization: Processing para =  90\n",
      "Retokenization: Processing para =  100\n",
      "Retokenization: Processing para =  110\n",
      "Retokenization: Processing para =  120\n",
      "Retokenization: Processing para =  130\n",
      "Retokenization: Processing para =  140\n",
      "Retokenization: Processing para =  150\n",
      "Retokenization: Processing para =  160\n",
      "Retokenization: Processing para =  170\n",
      "Retokenization: Processing para =  180\n",
      "Retokenization: Processing para =  190\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    from configparser import ConfigParser\n",
    "    config = ConfigParser()\n",
    "    config.read('../conf/collocations.ini') #read collocations configuration file\n",
    "    \n",
    "    #read the values from configuration file and store them in a dictionary for further usage\n",
    "    object_dict = dict()\n",
    "    \n",
    "    object_dict['input_data_path'] = config['Collocations']['input_data_path']\n",
    "    object_dict['output_data_path'] = config['Collocations']['output_data_path']\n",
    "    object_dict['input_filename'] = config['Collocations']['input_filename']\n",
    "    object_dict['encoding'] = config['Collocations']['encoding']\n",
    "    object_dict['ngram_results_dir'] = config['Collocations']['ngram_results_dir']\n",
    "    object_dict['pmi_results_dir'] = config['Collocations']['pmi_results_dir']\n",
    "    \n",
    "    object_dict['PMI_threshold'] = config['Collocations']['PMI_threshold']\n",
    "    \n",
    "    object_dict['ngram_processed_data'] = object_dict['input_filename'].split('.')[0] + '_NgramsProcessed.csv'\n",
    "    \n",
    "    print(object_dict)\n",
    "    \n",
    "    import os\n",
    "    try:\n",
    "        os.mkdir('../results')\n",
    "        os.mkdir('../results/Collocations_Results')\n",
    "        os.mkdir(object_dict['output_data_path'] + object_dict['ngram_results_dir'])\n",
    "        os.mkdir(object_dict['output_data_path'] + object_dict['pmi_results_dir'])\n",
    "        \n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    result = FindNgrams(object_dict) #Find ngrams from the corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
