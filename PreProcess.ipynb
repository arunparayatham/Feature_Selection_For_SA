{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run PreProcessConfiguration.py # running confie file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:(24999, 2)\n",
      "Drop Dupicates:(24901, 2)\n",
      "Drop Nulls: (24901, 2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\athik/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fbca8bb2a102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_PreProcessed.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPreProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m         \u001b[0mclasses_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mreviews_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-fbca8bb2a102>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj_dict)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_words_removal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gibberish_word_removal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# removes non english and noun words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgibberish_word_removal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemmatization'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# lemmatization of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-fbca8bb2a102>\u001b[0m in \u001b[0;36mgibberish_word_removal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;31m# Using a Tagger. Which is part-of-speech\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;31m# tagger or POS-tagger.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m                 \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordsList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m                 \u001b[1;31m#print(tagged)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             AP_MODEL_LOC = 'file:' + str(\n\u001b[1;32m--> 162\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'taggers/averaged_perceptron_tagger/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             )\n\u001b[0;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# Is the path item a zipfile?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpath_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from configparser import ConfigParser\n",
    "import pickle\n",
    "\n",
    "import enchant\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "class PreProcess():\n",
    "\n",
    "    def __init__(self,obj_dict):\n",
    "        try:\n",
    "            self.data=pd.read_csv(obj_dict['data'], encoding='latin1',header=0)\n",
    "        except:\n",
    "            print('Run first pre-processing configuration file (PreProcessConfiguration.py)')\n",
    "            \n",
    "        print(f\"Original:{self.data.shape}\")\n",
    "        data_dd = self.data.drop_duplicates()\n",
    "        dd = data_dd.reset_index(drop=True)\n",
    "        print(f\"Drop Dupicates:{dd.shape}\")\n",
    "        dd_dn = dd.dropna()\n",
    "        df = dd_dn.reset_index(drop=True)\n",
    "        print(f\"Drop Nulls: {df.shape}\")\n",
    "\n",
    "        self.lowercase=False\n",
    "        self.tokenized=False\n",
    "        self.punctuations=False\n",
    "        self.numberRemoval=False\n",
    "        self.classes=list(df['sentiment_class'])\n",
    "        self.reviews=list(df['text'])\n",
    "\n",
    "        if obj_dict['negation_handling']=='True': # converts dont to do not\n",
    "            self.negation_handling(obj_dict['appos'])\n",
    "        if obj_dict['remove_punctuation']=='True':# removes #!~ kind of litterals\n",
    "            self.remove_punctuations()\n",
    "        if obj_dict['number_removal']=='True': # removes numbers from text\n",
    "            self.number_removal()\n",
    "        if obj_dict['numbers_to_names']=='True': # converts 10 to ten (number to their english name)\n",
    "            self.numbers_to_names()\n",
    "        if obj_dict['stop_words_removal']=='True': # removes stop words\n",
    "            self.stop_words_removal()\n",
    "        if obj_dict['gibberish_word_removal']=='True': # removes non english and noun words\n",
    "            self.gibberish_word_removal()\n",
    "        if obj_dict['lemmatization']=='True': # lemmatization of words\n",
    "            self.lemmatization()\n",
    "        if obj_dict['convert_to_lowercase']=='True': # converting to lowercase\n",
    "            self.convert_to_lowercase()\n",
    "            \n",
    "        for i in range(len(self.reviews)): # removing extra spaces inthe reviews\n",
    "            self.reviews[i] = re.sub(r\"\\s+\", \" \", self.reviews[i])\n",
    "            \n",
    "        # removing one letter words\n",
    "        self.tokenize_text()\n",
    "        temp=[]\n",
    "        for i in range(len(self.reviews)):\n",
    "            temp=[j for j in self.reviews[i] if len(j)>1]\n",
    "            self.reviews[i]=' '.join(j for j in temp)\n",
    "            temp=[]\n",
    "        self.tokenized=False\n",
    "        \n",
    "        if obj_dict['tokenization']=='True': # tokenization\n",
    "            self.tokenize_text()\n",
    "\n",
    "        '''classes_df = pd.DataFrame(self.classes)\n",
    "        reviews_df = pd.DataFrame(self.reviews)\n",
    "\n",
    "        x=pd.concat([classes_df , reviews_df] , axis=1)\n",
    "        x.columns=['class','review']\n",
    "        x.to_csv('preprocessed_data.csv',index=False)'''\n",
    "        \n",
    "            \n",
    "    def gibberish_word_removal(self):\n",
    "        '''except for nouns removes words which are not in english dictionary'''\n",
    "        \n",
    "        standard_dict = enchant.Dict(\"en_US\")\n",
    "            # extracting each data row one by one\n",
    "        for i in range(len(self.reviews)):\n",
    "            tokenized = sent_tokenize(self.reviews[i])\n",
    "            for j in tokenized:\n",
    "                # Word tokenizers is used to find the words\n",
    "                # and punctuation in a string\n",
    "                wordsList = nltk.word_tokenize(j)\n",
    "                # removing stop words from wordList\n",
    "                # Using a Tagger. Which is part-of-speech\n",
    "                # tagger or POS-tagger.\n",
    "                tagged = nltk.pos_tag(wordsList)\n",
    "                #print(tagged)         \n",
    "\n",
    "            tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "            words_list = tokenizer.tokenize(self.reviews[i])\n",
    "            for word in words_list:\n",
    "                a = 0\n",
    "                if word.isnumeric() == False:\n",
    "                    #check if the word is a proper noun\n",
    "                    for j in tagged:\n",
    "                        if j[0] == word and (j[1] == 'NNP' or j[1] == 'NNPS'):\n",
    "                            a = 1\n",
    "                            break\n",
    "                    if a != 1 and standard_dict.check(word) == False:\n",
    "                        #get suggestions for the input word from the standard dictionary\n",
    "                        self.reviews[i] = self.reviews[i].replace(word, ' ')\n",
    "            \n",
    "            self.tokenized=False\n",
    "\n",
    "    def number_removal(self):\n",
    "        '''removes numerics'''\n",
    "        if self.numberRemoval==False:\n",
    "            for i in range(len(self.reviews)):\n",
    "                self.reviews[i] = re.sub(r'\\w*\\d\\w*', ' ', self.reviews[i])\n",
    "            self.numberRemoval=True\n",
    "\n",
    "    def tokenize_text(self):\n",
    "    \n",
    "        if self.tokenized==False:\n",
    "            for i in range(len(self.reviews)):\n",
    "                self.reviews[i]=word_tokenize(self.reviews[i])\n",
    "            self.tokenized=True\n",
    "\n",
    "    def negation_handling(self,appos):\n",
    "\n",
    "        ''' converts dont to do not'''\n",
    "        self.convert_to_lowercase()\n",
    "        self.remove_punctuations()\n",
    "        for i in range(len(self.reviews)):\n",
    "            for j in appos.keys():\n",
    "                self.reviews[i]=self.reviews[i].replace(j+' ',appos[j]+' ')\n",
    "\n",
    "    def convert_to_lowercase(self):\n",
    "\n",
    "        '''converts the text in data to lowercase'''\n",
    "        if self.lowercase==False:\n",
    "            for i in range(len(self.classes)):\n",
    "                self.classes[i]=self.classes[i].lower()\n",
    "                self.reviews[i]=self.reviews[i].lower()\n",
    "            self.lowercase=True\n",
    "\n",
    "    def remove_punctuations(self):\n",
    "\n",
    "        '''removes punctuations in text  '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' '''\n",
    "        if self.punctuations==False:\n",
    "            for i in range(len(self.reviews)):\n",
    "                self.reviews[i] = self.reviews[i].translate(str.maketrans('', '', string.punctuation))\n",
    "                self.punctuations=True\n",
    "\n",
    "    def numbers_to_names(self):\n",
    "\n",
    "        '''converts number(10) to its name(ten)'''\n",
    "        if self.numberRemoval==False:\n",
    "            self.convert_to_lowercase()\n",
    "            for i in range(len(self.reviews)):\n",
    "                m = re.findall(r'\\d+', self.reviews[i])\n",
    "                for j in m:\n",
    "                    try:\n",
    "                        x=num2words(j,lang='en_IN').replace('-',' ')\n",
    "                        self.reviews[i]=self.reviews[i].replace(j,x)\n",
    "                    except:\n",
    "                        print(f'for {i} review number exceeded that limit of abs')\n",
    "    \n",
    "    def stop_words_removal(self):\n",
    "        \n",
    "        self.convert_to_lowercase()\n",
    "        self.tokenize_text()\n",
    "        \n",
    "        my_stop_words = list(text.ENGLISH_STOP_WORDS) #importing stopwords\n",
    "        \n",
    "        #exclude the words from stop words which may effect sentiment of a sentence\n",
    "        exempt=['against','below','cannot','cant','couldnt','cry','fire','hasnt','never','not','nobody','nor'\n",
    "       ,'nothing','under','no','off','than']\n",
    "        \n",
    "        #include words which may be custom to your dataset\n",
    "        my_stop_words.extend(['url','https','http','com'])\n",
    "\n",
    "        for i in exempt:\n",
    "            my_stop_words.remove(i)\n",
    "            \n",
    "        #removing stopwords\n",
    "        for i in range(len(self.reviews)):\n",
    "            self.reviews[i] = [w for w in self.reviews[i] if not w in my_stop_words]\n",
    "            try:\n",
    "                self.reviews[i].remove(' ')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for i in range(len(self.reviews)):\n",
    "            self.reviews[i]=' '.join(w for w in self.reviews[i])+' '\n",
    "            \n",
    "        self.tokenized=False\n",
    "\n",
    "    def lemmatization(self):\n",
    "        \n",
    "        self.convert_to_lowercase()\n",
    "        for i in range(len(self.reviews)):\n",
    "            doc = nlp(self.reviews[i])\n",
    "            # Create list of tokens from given string\n",
    "            tokens = []\n",
    "            for token in doc:\n",
    "                tokens.append(token)\n",
    "\n",
    "            self.reviews[i] = \" \".join([token.lemma_ for token in doc])\n",
    "        \n",
    "        self.tokenized=False\n",
    "      \n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    \n",
    "    config = ConfigParser()\n",
    "    config.read('PreProcess.ini')\n",
    "\n",
    "    pfile = open('appos.pkl', 'rb')\n",
    "    apos = pickle.load(pfile)\n",
    "    pfile.close()\n",
    "    \n",
    "    obj_dict=dict()\n",
    "    obj_dict[\"data\"] =config['Data']['data']\n",
    "    obj_dict[\"negation_handling\"]=config['Data']['negation_handling']\n",
    "    obj_dict[\"remove_punctuation\"]=config['Data']['remove_punctuation']\n",
    "    obj_dict[\"numbers_to_names\"]=config['Data']['numbers_to_names']\n",
    "    obj_dict[\"stop_words_removal\"]=config['Data']['stop_words_removal']\n",
    "    obj_dict[\"lemmatization\"]=config['Data']['lemmatization']\n",
    "    obj_dict[\"convert_to_lowercase\"]=config['Data']['convert_to_lowercase']\n",
    "    obj_dict[\"tokenization\"]=config['Data']['tokenization']\n",
    "    obj_dict[\"number_removal\"]=config['Data']['number_removal']\n",
    "    obj_dict[\"gibberish_word_removal\"]=config['Data']['gibberish_word_removal']\n",
    "    obj_dict['appos']=apos\n",
    "    \n",
    "    if not os.path.exists(obj_dict[\"data\"].split('.')[0]+'_PreProcessed.csv'):\n",
    "        obj=PreProcess(obj_dict)\n",
    "        classes_df = pd.DataFrame(obj.classes)\n",
    "        reviews_df = pd.DataFrame(obj.reviews)\n",
    "        x=pd.concat([classes_df , reviews_df] , axis=1)\n",
    "        x.columns=['sentiment_class','text']\n",
    "        x.to_csv(obj_dict[\"data\"].split('.')[0]+'_PreProcessed.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
